{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pymongo\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyMongo to work with MongoDBs\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database and collection\n",
    "db = client.mars_db\n",
    "collection = db.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 89.0.4389\n",
      "[WDM] - Get LATEST driver version for 89.0.4389\n",
      "[WDM] - Driver [/Users/melissa/.wdm/drivers/chromedriver/mac64/89.0.4389.23/chromedriver] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "# This is not the given website, but I get returns, so I'm working with it for now\n",
    "news_url = (\"https://redplanetscience.com/#\")\n",
    "browser.visit(news_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML Object\n",
    "html = browser.html\n",
    "\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mars 2020 Stands on Its Own Six Wheels\n",
      "In time-lapse video, taken at JPL, captures the first time NASA's Mars 2020 rover carries its full weight on its legs and wheels.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the latest element that contains news title and news_paragraph\n",
    "title = soup.find('div', class_='content_title').text\n",
    "article_teaser = soup.find('div', class_='article_teaser_body').text\n",
    "\n",
    "# Display scrapped data \n",
    "print(title)\n",
    "print(article_teaser)\n",
    "\n",
    "# I think my issue was using find_all and .text together incorrectly and also not having the splinter set up.\n",
    "# I'm still unsure about how it all relates and why it won't work without the other, but at least I can start moving forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://spaceimages-mars.com/image/featured/mars2.jpg'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# JPL Mars image\n",
    "\n",
    "jpl_url = (\"https://spaceimages-mars.com/\")\n",
    "browser.visit(jpl_url)\n",
    "\n",
    "# Set up new instance of HTML object\n",
    "html = browser.html\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "response = soup.find('a', class_='showimg')\n",
    "\n",
    "# print(response)\n",
    "\n",
    "href = response['href']\n",
    "\n",
    "img_url = jpl_url + href\n",
    "\n",
    "img_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mars Facts\n",
    "https://galaxyfacts-mars.com/\n",
    "\n",
    "\n",
    "Visit the Mars Facts webpage here and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc.\n",
    "\n",
    "\n",
    "Use Pandas to convert the data to a HTML table string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try some table reading\n",
    "# Look into weird character issues\n",
    "facts_url = 'https://galaxyfacts-mars.com/'\n",
    "\n",
    "tables = pd.read_html(facts_url)\n",
    "tables\n",
    "\n",
    "mars_facts = tables[0]\n",
    "mars_facts\n",
    "\n",
    "mars_html = mars_facts.to_html('mars_facts.html')\n",
    "mars_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mars Hemispheres\n",
    "\n",
    "\n",
    "Visit the astrogeology site here to obtain high resolution images for each of Mars's hemispheres.\n",
    "\n",
    "\n",
    "You will need to click each of the links to the hemispheres in order to find the image url to the full resolution image.\n",
    "\n",
    "\n",
    "Save both the image url string for the full resolution hemisphere image, and the Hemisphere title containing the hemisphere name. Use a Python dictionary to store the data using the keys img_url and title.\n",
    "\n",
    "\n",
    "Append the dictionary with the image url string and the hemisphere title to a list. This list will contain one dictionary for each hemisphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the image urls for the given website\n",
    "# Ok, this works to pull one. How to get the other three?\n",
    "hemisphere_url = 'https://marshemispheres.com/'\n",
    "browser.visit(hemisphere_url)\n",
    "\n",
    "# Do not forget new instance!!!!\n",
    "html = browser.html\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "responses = soup.find_all('div', class_='item')\n",
    "# responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cerberus Hemisphere Enhanced',\n",
       "  'img_url': 'https://marshemispheres.com/images/f5e372a36edfa389625da6d0cc25d905_cerberus_enhanced.tif_full.jpg'},\n",
       " {'title': 'Schiaparelli Hemisphere Enhanced',\n",
       "  'img_url': 'https://marshemispheres.com/images/3778f7b43bbbc89d6e3cfabb3613ba93_schiaparelli_enhanced.tif_full.jpg'},\n",
       " {'title': 'Syrtis Major Hemisphere Enhanced',\n",
       "  'img_url': 'https://marshemispheres.com/images/555e6403a6ddd7ba16ddb0e471cadcf7_syrtis_major_enhanced.tif_full.jpg'},\n",
       " {'title': 'Valles Marineris Hemisphere Enhanced',\n",
       "  'img_url': 'https://marshemispheres.com/images/b3c7c6c9138f57b4756be9b9c43e3a48_valles_marineris_enhanced.tif_full.jpg'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect the image urls for the given website\n",
    "# Why does it work with h3 and not h2?\n",
    "hemisphere_url = 'https://marshemispheres.com/'\n",
    "browser.visit(hemisphere_url)\n",
    "\n",
    "# Do not forget new instance!!!!\n",
    "html = browser.html\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retreive all items that contain mars hemispheres information\n",
    "images = soup.find_all('div', class_='item')\n",
    "\n",
    "# Create empty list for hemisphere urls \n",
    "image_urls = []\n",
    "\n",
    "# Store the main_ul \n",
    "main_url = 'https://marshemispheres.com/'\n",
    "\n",
    "# Loop through the items previously stored\n",
    "for image in images: \n",
    "    \n",
    "    # Grab link to image to set up nested browser.html \n",
    "    partial_img_url = image.find('a', class_='itemLink product-item')['href']\n",
    "    \n",
    "    # Visit the link that contains the full image website \n",
    "    browser.visit(main_url + partial_img_url)\n",
    "    \n",
    "    # HTML Object for site visited in nest \n",
    "    partial_img_html = browser.html\n",
    "    \n",
    "    # Parse HTML with Beautiful Soup for every nested hemisphere information website \n",
    "    soup = BeautifulSoup(partial_img_html, 'html.parser')\n",
    "    \n",
    "    # Retrieve full image source \n",
    "    img_url = main_url + soup.find('img', class_='wide-image')['src']\n",
    "    \n",
    "    title = soup.find('h2', class_='title').text\n",
    "    \n",
    "    # Append the retreived information into a list of dictionaries \n",
    "    image_urls.append({\"title\" : title, \"img_url\" : img_url})\n",
    "    \n",
    "\n",
    "# Display hemisphere_image_urls\n",
    "image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
