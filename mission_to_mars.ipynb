{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pymongo\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyMongo to work with MongoDBs\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database and collection\n",
    "db = client.mars_db\n",
    "collection = db.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 89.0.4389\n",
      "[WDM] - Get LATEST driver version for 89.0.4389\n",
      "[WDM] - Driver [/Users/melissa/.wdm/drivers/chromedriver/mac64/89.0.4389.23/chromedriver] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "# This is not the given website, but I get returns, so I'm working with it for now\n",
    "news_url = (\"https://redplanetscience.com/#\")\n",
    "browser.visit(news_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML Object\n",
    "html = browser.html\n",
    "\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASA to Broadcast Mars 2020 Perseverance Launch, Prelaunch Activities\n",
      "Starting July 27, news activities will cover everything from mission engineering and science to returning samples from Mars to, of course, the launch itself.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the latest element that contains news title and news_paragraph\n",
    "title = soup.find('div', class_='content_title').text\n",
    "article_teaser = soup.find('div', class_='article_teaser_body').text\n",
    "\n",
    "# Display scrapped data \n",
    "print(title)\n",
    "print(article_teaser)\n",
    "\n",
    "# I think my issue was using find_all and .text together incorrectly and also not having the splinter set up.\n",
    "# I'm still unsure about how it all relates and why it won't work without the other, but at least I can start moving forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JPL Mars image\n",
    "\n",
    "jpl_url = (\"https://spaceimages-mars.com/\")\n",
    "browser.visit(jpl_url)\n",
    "\n",
    "# Set up new instance of HTML object\n",
    "html = browser.html\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Gather featured_image_url\n",
    "# Yea, nothing is woring\n",
    "# img-card-top\n",
    "\n",
    "img_url = soup.find('img', class_='headerimage fade in')\n",
    "\n",
    "print(img_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://spaceimages-mars.com/image/featured/mars1.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pull image link\n",
    "# pic_src = []\n",
    "# for image in images:\n",
    "#     pic = image['data-fancybox-href']\n",
    "#     pic_src.append(pic)\n",
    "\n",
    "# featured_image_url = 'https://www.jpl.nasa.gov' + pic\n",
    "# featured_image_url\n",
    "\n",
    "# <a target=\"_blank\" class=\"showimg fancybox-thumbs\" href=\"image/featured/mars3.jpg\"> \n",
    "# <button class=\"btn btn-outline-light\"> FULL IMAGE</button></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mars Facts\n",
    "https://galaxyfacts-mars.com/\n",
    "\n",
    "\n",
    "Visit the Mars Facts webpage here and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc.\n",
    "\n",
    "\n",
    "Use Pandas to convert the data to a HTML table string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html(url)\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve page with the requests module\n",
    "response = requests.get(jpl_url)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# soup\n",
    "\n",
    "\n",
    "# pull images from website\n",
    "images = soup.find_all('a', class_=\"fancybox\")\n",
    "#print(images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
